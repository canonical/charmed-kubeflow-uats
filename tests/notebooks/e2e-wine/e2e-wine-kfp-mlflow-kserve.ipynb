{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b332dcf3-b5e7-4e42-9750-c5e40e5af9b8",
   "metadata": {},
   "source": [
    "# End-to-End UAT Test: Wine Quality Predictor\n",
    "\n",
    "This notebook demonstrates a complete end-to-end machine learning pipeline using Kubeflow, MLflow, and KServe. The pipeline covers the following steps:\n",
    "1. **Data Ingestion**: Downloading a wine quality dataset from a public URL.\n",
    "2. **Data Preprocessing**: Cleaning and transforming the dataset into a format suitable for model training.\n",
    "3. **Model Training**: Training an ElasticNet regression model to predict wine quality, with automatic logging of model artifacts to MLflow.\n",
    "4. **Model Deployment**: Deploying the trained model as a scalable inference service using KServe.\n",
    "5. **Model Inference**: Making predictions on new data using the deployed model and verifying the end-to-end functionality.\n",
    "6. **Cleanup**: Removing the deployed inference service after the test is completed to free up resources.\n",
    "\n",
    "This UAT test serves as a demonstration of the seamless integration of Kubeflow Pipelines with MLflow for model management and KServe for model deployment, along with proper resource management by cleaning up the deployed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c8729-843c-45f1-80ad-da8eed206212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c9c41-a334-4aa6-b84b-ce1b66dcadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import mlflow\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from kfp.dsl import Input, Model, component\n",
    "from kfp.dsl import InputPath, OutputPath, pipeline, component\n",
    "from kserve import KServeClient\n",
    "from mlflow.tracking import MlflowClient\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_PROXY = HTTPS_PROXY = NO_PROXY = None\n",
    "\n",
    "if os.environ.get('HTTP_PROXY') and os.environ.get('HTTPS_PROXY') and os.environ.get('NO_PROXY'):\n",
    "    HTTP_PROXY = os.environ['HTTP_PROXY']\n",
    "    HTTPS_PROXY = os.environ['HTTPS_PROXY']\n",
    "    # add `.kubeflow` to NO_PROXY needed for pipelines\n",
    "    NO_PROXY = os.environ['NO_PROXY']\n",
    "\n",
    "def add_proxy(obj, http_proxy=HTTP_PROXY, https_proxy=HTTPS_PROXY, no_proxy=NO_PROXY):\n",
    "    \"\"\"Adds the proxy env vars to the PipelineTask object.\"\"\"\n",
    "    return (\n",
    "        obj.set_env_variable(name='http_proxy', value=http_proxy)\n",
    "        .set_env_variable(name='https_proxy', value=https_proxy)\n",
    "        .set_env_variable(name='HTTP_PROXY', value=http_proxy)\n",
    "        .set_env_variable(name='HTTPS_PROXY', value=https_proxy)\n",
    "        .set_env_variable(name='no_proxy', value=no_proxy)\n",
    "        .set_env_variable(name='NO_PROXY', value=no_proxy)\n",
    "    )\n",
    "\n",
    "def proxy_envs_set() -> bool:\n",
    "    if HTTP_PROXY and HTTPS_PROXY and NO_PROXY:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d166385-f03f-424a-81bd-9676e2a4b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant for the Inference Service name\n",
    "ISVC_NAME = \"wine-regressor3\"\n",
    "MLFLOW_RUN_NAME = \"elastic_net_models\"\n",
    "MLFLOW_MODEL_NAME = \"wine-elasticnet\"\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",  # Use Python 3.11 base image\n",
    "    packages_to_install=[\"requests==2.32.3\", \"pandas==2.2.2\"]\n",
    ")\n",
    "def download_dataset(url: str, dataset_path: OutputPath('Dataset')) -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # Download the dataset from the provided URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Convert the response content to a Pandas DataFrame\n",
    "    from io import StringIO\n",
    "    dataset = pd.read_csv(StringIO(response.text), header=0, sep=\";\")\n",
    "\n",
    "    # Save the DataFrame to a CSV file at the specified output path\n",
    "    dataset.to_csv(dataset_path, index=False)\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",  # Use Python 3.11 base image\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"pyarrow==15.0.2\"]\n",
    ")\n",
    "def preprocess_dataset(dataset: InputPath('Dataset'), output_file: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(dataset, header=0)\n",
    "    \n",
    "    # Preprocess the DataFrame by standardizing column names\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    \n",
    "    # Save the preprocessed DataFrame as a Parquet file\n",
    "    df.to_parquet(output_file)\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",  # Use Python 3.11 base image\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"scikit-learn==1.5.1\", \"mlflow==2.15.1\", \"pyarrow==15.0.2\", \"boto3==1.34.162\"]\n",
    ")\n",
    "def train_model(dataset: InputPath('Dataset'), run_name: str, model_name: str) -> str:\n",
    "    import os\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load the preprocessed dataset\n",
    "    df = pd.read_parquet(dataset)\n",
    "    \n",
    "    # Define the target column for prediction\n",
    "    target_column = \"quality\"\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        df.drop(columns=[target_column]),\n",
    "        df[target_column], test_size=0.25,\n",
    "        random_state=42, stratify=df[target_column]\n",
    "    )\n",
    "\n",
    "    # Enable MLflow auto logging for scikit-learn models\n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    # Start an MLflow run and train the model\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        mlflow.set_tag(\"author\", \"kf-testing\")\n",
    "        lr = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
    "        lr.fit(train_x, train_y)\n",
    "        mlflow.sklearn.log_model(lr, \"model\", registered_model_name=model_name)\n",
    "        \n",
    "        # Return the model artifact URI as a string\n",
    "        model_uri = f\"{run.info.artifact_uri}/model\"\n",
    "        print(model_uri)\n",
    "        return model_uri\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",  # Use Python 3.11 base image\n",
    "    packages_to_install=[\"kserve==0.13.1\", \"kubernetes==26.1.0\", \"tenacity==9.0.0\"]\n",
    ")\n",
    "def deploy_model_with_kserve(model_uri: str, isvc_name: str) -> str:\n",
    "    from kubernetes.client import V1ObjectMeta\n",
    "    from kserve import (\n",
    "        constants,\n",
    "        KServeClient,\n",
    "        V1beta1InferenceService,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1SKLearnSpec,\n",
    "    )\n",
    "    from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "    # Initialize the Inference Service specification\n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=V1ObjectMeta(\n",
    "            name=isvc_name,\n",
    "            annotations={\"sidecar.istio.io/inject\": \"false\"},\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"kserve-controller-s3\",\n",
    "                sklearn=V1beta1SKLearnSpec(\n",
    "                    storage_uri=model_uri\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Deploy the Inference Service using KServe\n",
    "    client = KServeClient()\n",
    "    client.create(isvc)\n",
    "\n",
    "    # Retry logic to ensure the Inference Service is ready\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "        stop=stop_after_attempt(30),\n",
    "        reraise=True,\n",
    "    )\n",
    "    def assert_isvc_created(client, isvc_name):\n",
    "        assert client.is_isvc_ready(isvc_name), f\"Failed to create Inference Service {isvc_name}.\"\n",
    "\n",
    "    # Wait until the service is ready and get the service URL\n",
    "    assert_isvc_created(client, isvc_name)\n",
    "    isvc_resp = client.get(isvc_name)\n",
    "    isvc_url = isvc_resp['status']['address']['url']\n",
    "    print(\"Inference URL:\", isvc_url)\n",
    "    \n",
    "    return isvc_url\n",
    "\n",
    "# Fetch environment variables for MLflow tracking and AWS credentials\n",
    "# These are guaranteed to be present because of the mlflow's poddefault please refer to [this guide](https://documentation.ubuntu.com/charmed-mlflow/en/latest/tutorial/mlflow-kubeflow/\n",
    "\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "mlflow_s3_endpoint_url = os.getenv('MLFLOW_S3_ENDPOINT_URL')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "@pipeline(name='download-preprocess-train-deploy-pipeline')\n",
    "def download_preprocess_train_deploy_pipeline(url: str):\n",
    "    # Step 1: Download the dataset from the URL\n",
    "    download_task = download_dataset(url=url)\n",
    "    \n",
    "    # Step 2: Preprocess the downloaded dataset\n",
    "    preprocess_task = preprocess_dataset(\n",
    "        dataset=download_task.outputs['dataset_path']\n",
    "    )\n",
    "    \n",
    "    # Step 3: Train the model on the preprocessed dataset\n",
    "    train_task = train_model(\n",
    "        dataset=preprocess_task.outputs['output_file'], run_name=MLFLOW_RUN_NAME, model_name=MLFLOW_MODEL_NAME\n",
    "    ).set_env_variable(name='MLFLOW_TRACKING_URI', value=mlflow_tracking_uri)\\\n",
    "     .set_env_variable(name='MLFLOW_S3_ENDPOINT_URL', value=mlflow_s3_endpoint_url)\\\n",
    "     .set_env_variable(name='AWS_ACCESS_KEY_ID', value=aws_access_key_id)\\\n",
    "     .set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\n",
    "    \n",
    "    # Step 4: Deploy the trained model with KServe\n",
    "    deploy_task = deploy_model_with_kserve(\n",
    "        model_uri=train_task.output, isvc_name=ISVC_NAME\n",
    "    ).set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\n",
    "\n",
    "# This pipeline definition is identical to the one above with the only difference being\n",
    "# that environment variables are added to each step of the pipeline, in order to enable them\n",
    "# to run behind a proxy. Which pipeline is used is defined in the next cell according to if \n",
    "# such environment variables are set.\n",
    "@pipeline(name='download-preprocess-train-deploy-pipeline')\n",
    "def download_preprocess_train_deploy_pipeline_proxy(url: str):\n",
    "    # Step 1: Download the dataset from the URL\n",
    "    download_task = add_proxy(download_dataset(url=url))\n",
    "    \n",
    "    # Step 2: Preprocess the downloaded dataset\n",
    "    preprocess_task =  add_proxy(preprocess_dataset(\n",
    "        dataset=download_task.outputs['dataset_path']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Step 3: Train the model on the preprocessed dataset\n",
    "    train_task = add_proxy(train_model(\n",
    "        dataset=preprocess_task.outputs['output_file'], run_name=MLFLOW_RUN_NAME, model_name=MLFLOW_MODEL_NAME\n",
    "    ).set_env_variable(name='MLFLOW_TRACKING_URI', value=mlflow_tracking_uri)\\\n",
    "     .set_env_variable(name='MLFLOW_S3_ENDPOINT_URL', value=mlflow_s3_endpoint_url)\\\n",
    "     .set_env_variable(name='AWS_ACCESS_KEY_ID', value=aws_access_key_id)\\\n",
    "     .set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\n",
    "    )\n",
    "    \n",
    "    # Step 4: Deploy the trained model with KServe\n",
    "    deploy_task = add_proxy(deploy_model_with_kserve(\n",
    "        model_uri=train_task.output, isvc_name=ISVC_NAME\n",
    "    ).set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdeb7a-2898-4a41-bba3-7f2b5c5b171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a KFP client\n",
    "# This client is used to interact with the Kubeflow Pipelines API.\n",
    "client = kfp.Client()\n",
    "\n",
    "# Define the URL for the dataset\n",
    "# This URL points to the dataset that will be downloaded and processed in the pipeline.\n",
    "url = 'https://raw.githubusercontent.com/canonical/kubeflow-examples/main/e2e-wine-kfp-mlflow/winequality-red.csv'\n",
    "\n",
    "# If proxy environment variables are set, use the `_proxy` pipeline definition.\n",
    "if proxy_envs_set():\n",
    "    # Compile the pipeline to a YAML file\n",
    "    # This step translates the Python-based pipeline definition into a YAML file \n",
    "    # that can be used to run the pipeline in Kubeflow Pipelines.\n",
    "    kfp.compiler.Compiler().compile(download_preprocess_train_deploy_pipeline_proxy, 'download_preprocess_train_deploy_pipeline_proxy.yaml')\n",
    "    # Run the pipeline\n",
    "    # This command starts a new run of the compiled pipeline, passing in the dataset URL as an argument.\n",
    "    run = client.create_run_from_pipeline_func(download_preprocess_train_deploy_pipeline_proxy, arguments={'url': url})\n",
    "else:\n",
    "    # Compile the pipeline to a YAML file\n",
    "    # This step translates the Python-based pipeline definition into a YAML file \n",
    "    # that can be used to run the pipeline in Kubeflow Pipelines.\n",
    "    kfp.compiler.Compiler().compile(download_preprocess_train_deploy_pipeline, 'download_preprocess_train_deploy_pipeline.yaml')\n",
    "    # Run the pipeline\n",
    "    # This command starts a new run of the compiled pipeline, passing in the dataset URL as an argument.\n",
    "    run = client.create_run_from_pipeline_func(download_preprocess_train_deploy_pipeline, arguments={'url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256137a6-8a1c-4c4d-a544-9f716b1fbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "    stop=stop_after_attempt(90),\n",
    "    reraise=True,\n",
    ")\n",
    "def assert_kfp_run_succeeded(client, run_id):\n",
    "    \"\"\"Wait for the run to complete successfully.\"\"\"\n",
    "    run = client.get_run(run_id=run_id)\n",
    "    state = run.state  # Assuming the status is directly under run object in V2beta1Run\n",
    "    assert state == \"SUCCEEDED\", f\"KFP run is in {state} state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e2e04-394d-44d0-ace9-56e3435c86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_kfp_run_succeeded(client, run.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd6402-3edc-4582-a042-f2315d0cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KServe client\n",
    "# This client is used to interact with the KServe Inference Service.\n",
    "kserve_client = KServeClient()\n",
    "\n",
    "# Retrieve the Inference Service details\n",
    "# Fetches the Inference Service by name and extracts the URL for making predictions.\n",
    "isvc_resp = kserve_client.get(ISVC_NAME)\n",
    "inference_service_url = isvc_resp['status']['address']['url']\n",
    "print(\"Inference URL:\", inference_service_url)\n",
    "\n",
    "# Define the input data for prediction\n",
    "# This data matches the expected input format of the deployed model, with each instance being a list of feature values.\n",
    "input_data = {\n",
    "    \"instances\": [\n",
    "        [7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4],\n",
    "        [7.8, 0.88, 0.0, 2.6, 0.098, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send a prediction request to the Inference Service\n",
    "# This sends the input data to the model for prediction via a POST request and prints the response.\n",
    "response = requests.post(f\"{inference_service_url}/v1/models/{ISVC_NAME}:predict\", json=input_data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29094980-dbdd-46ef-a793-37d7148db0a4",
   "metadata": {},
   "source": [
    "## Delete Inference Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafef13-63fa-4a0a-868e-82c70e3c7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "kserve_client.delete(ISVC_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35b016-6857-40e6-83b1-cf611a6d58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "    stop=stop_after_attempt(30),\n",
    "    reraise=True,\n",
    ")\n",
    "def assert_isvc_deleted(kserve_client, isvc_name):\n",
    "    \"\"\"Wait for the Inference Service to be deleted.\"\"\"\n",
    "    try:\n",
    "        # try fetching the ISVC to verify it was deleted successfully\n",
    "        isvc = kserve_client.get(isvc_name)\n",
    "        assert not isvc, f\"Failed to delete Inference Service {isvc_name}!\"\n",
    "    except RuntimeError as err:\n",
    "        assert \"Not Found\" in str(err), f\"Caught unexpected exception: {err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b896d1d-0207-4cc8-a2e1-a2138be89ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_isvc_deleted(kserve_client, ISVC_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d6a98-f051-4dca-9477-8ce455ab13fa",
   "metadata": {},
   "source": [
    "# Delete MLflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e15fe-3b44-46d1-89d6-622bc5cdc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# Remove a registered model\n",
    "client.delete_registered_model(name=MLFLOW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb49c25-7a45-43a8-9b61-592ead165255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
