{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba70a2ba-3645-419b-8f8d-3c75e5864af1",
   "metadata": {},
   "source": [
    "# UAT to ensure Kubeflow pipelines can access Spark \n",
    "\n",
    "This notebook verifies that the Kubeflow pipeline stages are able to run Spark jobs. This notebook creates a component that runs a trivial Spark job to calculate the number of vowels in a sample string, and then a pipeline that executes this component.\n",
    "\n",
    "The expected outcome is that the Spark job inside the component runs successfully, and this is verified by polling the run status. In the component, an assertion is made for the correct result, and if the run was successful, it means that the assertion passed. In the event of failure of assertion, the run would be in FAILURE state.\n",
    "\n",
    "This notebook requires Kubeflow + Spark setup to have been deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2e9c5",
   "metadata": {},
   "source": [
    "### Install necessary packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f75e55-7bad-44e7-a65f-aedc81734a48",
   "metadata": {
    "tags": [
     "pytest-skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Please check the requirements.in file for more details\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9436c3",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd7548-bae9-4430-b548-f420d72a8aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp import dsl, kubernetes\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529afc5",
   "metadata": {},
   "source": [
    "### Define literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643563e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"Count the number of vowels in a given string\"\n",
    "CHARMED_SPARK_OCI_IMAGE = \"ghcr.io/canonical/charmed-spark:3.5.5-22.04_edge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c821b",
   "metadata": {},
   "source": [
    "### Prepare proxy wrapper utilities if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45466795",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_PROXY = HTTPS_PROXY = NO_PROXY = None\n",
    "\n",
    "if os.environ.get(\"HTTP_PROXY\") and os.environ.get(\"HTTPS_PROXY\") and os.environ.get(\"NO_PROXY\"):\n",
    "    HTTP_PROXY = os.environ[\"HTTP_PROXY\"]\n",
    "    HTTPS_PROXY = os.environ[\"HTTPS_PROXY\"]\n",
    "    NO_PROXY = os.environ[\"NO_PROXY\"]\n",
    "\n",
    "\n",
    "def add_proxy(obj, http_proxy=HTTP_PROXY, https_proxy=HTTPS_PROXY, no_proxy=NO_PROXY):\n",
    "    \"\"\"Adds the proxy env vars to the PipelineTask object.\"\"\"\n",
    "    return (\n",
    "        obj.set_env_variable(name=\"http_proxy\", value=http_proxy)\n",
    "        .set_env_variable(name=\"https_proxy\", value=https_proxy)\n",
    "        .set_env_variable(name=\"HTTP_PROXY\", value=http_proxy)\n",
    "        .set_env_variable(name=\"HTTPS_PROXY\", value=https_proxy)\n",
    "        .set_env_variable(name=\"no_proxy\", value=no_proxy)\n",
    "        .set_env_variable(name=\"NO_PROXY\", value=no_proxy)\n",
    "    )\n",
    "\n",
    "\n",
    "def proxy_envs_set() -> bool:\n",
    "    if HTTP_PROXY and HTTPS_PROXY and NO_PROXY:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be562d14",
   "metadata": {},
   "source": [
    "### Create Spark test component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=CHARMED_SPARK_OCI_IMAGE,\n",
    ")\n",
    "def spark_test_component() -> None:\n",
    "    import logging\n",
    "    import os\n",
    "    from operator import add\n",
    "    from spark8t.session import SparkSession\n",
    "\n",
    "    def count_vowels(text: str) -> int:\n",
    "        \"\"\"Function that counts vowels in the given string.\"\"\"\n",
    "        count = 0\n",
    "        for char in text:\n",
    "            if char.lower() in \"aeiou\":\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    lines = \"\"\"Canonical's Charmed Data Platform solution for Apache Spark runs Spark jobs on your Kubernetes cluster.\n",
    "    You can get started right away with MicroK8s - the mightiest tiny Kubernetes distro around! \n",
    "    The spark-client snap simplifies the setup process to run Spark jobs against your Kubernetes cluster. \n",
    "    Spark on Kubernetes is a complex environment with many moving parts.\n",
    "    Sometimes, small mistakes can take a lot of time to debug and figure out.\n",
    "    \"\"\"\n",
    "\n",
    "    SPARK_SERVICE_ACCOUNT = os.environ[\"SPARK_SERVICE_ACCOUNT\"]\n",
    "    SPARK_NAMESPACE = os.environ[\"SPARK_NAMESPACE\"]\n",
    "\n",
    "    with SparkSession(\n",
    "        app_name=\"CountVowels\", namespace=SPARK_NAMESPACE, username=SPARK_SERVICE_ACCOUNT\n",
    "    ) as session:\n",
    "        n = session.sparkContext.parallelize(lines.splitlines(), 2).map(count_vowels).reduce(add)\n",
    "        assert n == count_vowels(lines)\n",
    "\n",
    "    logging.warning(f\"The number of vowels in the string is {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928195fd",
   "metadata": {},
   "source": [
    "### Create a KF Pipeline (without proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56376a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"spark-test-pipeline\")\n",
    "def spark_pipeline():\n",
    "    task = spark_test_component()\n",
    "    kubernetes.pod_metadata.add_pod_label(\n",
    "        task,\n",
    "        label_key=\"access-spark-pipeline\",\n",
    "        label_value=\"true\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d39c5e",
   "metadata": {},
   "source": [
    "### Create KF Pipeline (with proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a019606",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"spark-test-pipeline\")\n",
    "def spark_pipeline_proxy():\n",
    "    task = add_proxy(spark_test_component())\n",
    "    kubernetes.pod_metadata.add_pod_label(\n",
    "        task,\n",
    "        label_key=\"access-spark-pipeline\",\n",
    "        label_value=\"true\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574af02c",
   "metadata": {},
   "source": [
    "### Create KFP client and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f096b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "# Select appropriate pipeline based on proxy environment variables\n",
    "if proxy_envs_set():\n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        spark_pipeline_proxy, arguments={}, experiment_name=EXPERIMENT_NAME, enable_caching=False\n",
    "    )\n",
    "else:\n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        spark_pipeline, arguments={}, experiment_name=EXPERIMENT_NAME, enable_caching=False\n",
    "    )\n",
    "\n",
    "# See the experiment and run status\n",
    "client.list_experiments().experiments\n",
    "client.get_run(run.run_id).state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af23428",
   "metadata": {},
   "source": [
    "### Define assertion to check that the run was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "    stop=stop_after_attempt(30),\n",
    "    reraise=True,\n",
    ")\n",
    "def assert_run_succeeded(client, run_id):\n",
    "    \"\"\"Wait for the run to complete successfully.\"\"\"\n",
    "    status = client.get_run(run_id).state\n",
    "    assert status == \"SUCCEEDED\", f\"KFP run in {status} state.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e43ce",
   "metadata": {},
   "source": [
    "### Assert that the run was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch KFP experiment to ensure it exists\n",
    "client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "assert_run_succeeded(client, run.run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
