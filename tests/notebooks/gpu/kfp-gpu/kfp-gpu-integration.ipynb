{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test KFP Integration\n",
    "\n",
    "- create an experiment\n",
    "- create a run\n",
    "- check run passes\n",
    "\n",
    "This Notebook is based on the condition Kubeflow pipelines [example](https://github.com/kubeflow/pipelines/blob/master/samples/core/condition/condition_v2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please check the requirements.in file for more details\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Check GPU' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"kubeflownotebookswg/jupyter-tensorflow-cuda-full:v1.9.0\")\n",
    "def check_gpu() -> str:\n",
    "    \"\"\"Check GPU\"\"\"\n",
    "    import tensorflow as tf\n",
    "    gpu_list = tf.config.list_physical_devices('GPU')\n",
    "    print(\"GPU list:\", gpu_list)\n",
    "    return str(len(gpu_list)>0)\n",
    "\n",
    "def configure_check_gpu(obj):\n",
    "    \"\"\"Adds the proxy env vars to the PipelineTask object.\"\"\"\n",
    "    return ( obj.add_node_selector_constraint(accelerator = \"nvidia.com/gpu\").set_accelerator_limit(limit = 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def check_gpu_pipeline() -> str:\n",
    "    \"\"\"My ML pipeline.\"\"\"\n",
    "    check_gpu1 = check_gpu()\n",
    "    configure_check_gpu(check_gpu1)\n",
    "    return check_gpu1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting enable_caching to False to overcome https://github.com/canonical/bundle-kubeflow/issues/1067\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    check_gpu_pipeline,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments().experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_run(run.run_id).state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "    stop=stop_after_attempt(30),\n",
    "    reraise=True,\n",
    ")\n",
    "def assert_run_succeeded(client, run_id):\n",
    "    \"\"\"Wait for the run to complete successfully.\"\"\"\n",
    "    status = client.get_run(run_id).state\n",
    "    assert status == \"SUCCEEDED\", f\"KFP run in {status} state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch KFP experiment to ensure it exists\n",
    "client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "assert_run_succeeded(client, run.run_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
