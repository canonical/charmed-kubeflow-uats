{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KFP Integration\n",
    "\n",
    "- create an experiment\n",
    "- create a run\n",
    "- check that the run passes. This happens only when both of the following are true:\n",
    "    * the run's pod is scheduled on a node with an NVIDIA GPU\n",
    "    * the code, and more specifically Tensorflow framework, has access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please check the requirements.in file for more details\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Check access to GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_PROXY = HTTPS_PROXY = NO_PROXY = None\n",
    "\n",
    "if os.environ.get('HTTP_PROXY') and os.environ.get('HTTPS_PROXY') and os.environ.get('NO_PROXY'):\n",
    "    HTTP_PROXY = os.environ['HTTP_PROXY']\n",
    "    HTTPS_PROXY = os.environ['HTTPS_PROXY']\n",
    "    NO_PROXY = os.environ['NO_PROXY']\n",
    "\n",
    "def add_proxy(task: dsl.PipelineTask, http_proxy=HTTP_PROXY, https_proxy=HTTPS_PROXY, no_proxy=NO_PROXY) -> dsl.PipelineTask:\n",
    "    \"\"\"Adds the proxy env vars to the PipelineTask object.\"\"\"\n",
    "    return (\n",
    "        task.set_env_variable(name='http_proxy', value=http_proxy)\n",
    "        .set_env_variable(name='https_proxy', value=https_proxy)\n",
    "        .set_env_variable(name='HTTP_PROXY', value=http_proxy)\n",
    "        .set_env_variable(name='HTTPS_PROXY', value=https_proxy)\n",
    "        .set_env_variable(name='no_proxy', value=no_proxy)\n",
    "        .set_env_variable(name='NO_PROXY', value=no_proxy)\n",
    "    )\n",
    "\n",
    "def proxy_envs_set():\n",
    "    \"\"\"Check if the proxy env vars are set\"\"\"\n",
    "    if HTTP_PROXY and HTTPS_PROXY and NO_PROXY:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"kubeflownotebookswg/jupyter-tensorflow-cuda:v1.9.0\")\n",
    "def gpu_check() -> str:\n",
    "    \"\"\"Check access to a GPU.\"\"\"\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"GPU list:\", gpus)\n",
    "    if not gpus:\n",
    "        raise RuntimeError(\"No GPU has been detected.\")\n",
    "    return str(len(gpus)>0)\n",
    "\n",
    "def add_gpu_request(task: dsl.PipelineTask) -> dsl.PipelineTask:\n",
    "    \"\"\"Add a request field for a GPU to the container created by the PipelineTask object.\"\"\"\n",
    "    return ( task.add_node_selector_constraint(accelerator = \"nvidia.com/gpu\").set_accelerator_limit(limit = 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def gpu_check_pipeline() -> str:\n",
    "    \"\"\"Create a pipeline that runs code to check access to a GPU.\"\"\"\n",
    "    gpu_check1 = add_gpu_request(gpu_check())\n",
    "    return gpu_check1.output\n",
    "\n",
    "@dsl.pipeline\n",
    "def gpu_check_pipeline_proxy() -> str:\n",
    "    \"\"\"Create a pipeline that runs code to check access to a GPU and sets the appropriate proxy ENV variables.\"\"\"\n",
    "    gpu_check1 = add_proxy(add_gpu_request(gpu_check()))\n",
    "    return gpu_check1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting enable_caching to False to overcome https://github.com/canonical/bundle-kubeflow/issues/1067\n",
    "if proxy_envs_set():\n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        gpu_check_pipeline_proxy,\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        enable_caching=False,\n",
    "    )\n",
    "else:\n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        gpu_check_pipeline,\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        enable_caching=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments().experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_run(run.run_id).state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
    "    stop=stop_after_attempt(30),\n",
    "    reraise=True,\n",
    ")\n",
    "def assert_run_succeeded(client, run_id):\n",
    "    \"\"\"Wait for the run to complete successfully.\"\"\"\n",
    "    status = client.get_run(run_id).state\n",
    "    assert status == \"SUCCEEDED\", f\"KFP run in {status} state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch KFP experiment to ensure it exists\n",
    "client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "assert_run_succeeded(client, run.run_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
